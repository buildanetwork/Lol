{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\anoth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import text_to_word_sequence,one_hot,Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True,limit=450000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "label=[]\n",
    "with open('train.tsv') as tsvfile:\n",
    "  reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "  for row in reader:\n",
    "        r=[]\n",
    "        if len(row[2].split())<=25 and row[3]!='2':\n",
    "                for i in (row[2].split()):\n",
    "                    try:\n",
    "                        r.append(model[i])\n",
    "                    except KeyError:\n",
    "                        r.append(np.zeros(300))\n",
    "                k=len(r)\n",
    "                if k<25:\n",
    "                        for i in range(0,25-k):\n",
    "                            r.append(np.zeros(300))\n",
    "                if (row[3]=='0' or row[3]=='1'):\n",
    "                        label.append(0)\n",
    "                elif (row[3]=='3' or row[3]=='4'):\n",
    "                        label.append(1)\n",
    "                data.append(r)\n",
    "del model\n",
    "del data[0]                \n",
    "traindata=np.array(data[:10000])\n",
    "testdata=np.array(data[10000:20000])\n",
    "del data\n",
    "trainlabel=np.array(label[:10000])\n",
    "testlabel=np.array(label[10000:20000])\n",
    "del label\n",
    "print(len(traindata))\n",
    "print(len(trainlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnmodel(features, labels, mode):    \n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 25,300,1])\n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer,filters=50,kernel_size=[4, 5],strides=5,activation=tf.nn.relu)\n",
    "    print(conv1.shape)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    print(pool1.shape)\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,filters=100,kernel_size=[5, 6],padding=\"same\",activation=tf.nn.relu)\n",
    "    print(conv2.shape)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    print(pool2.shape)\n",
    "    conv3 = tf.layers.conv2d(inputs=pool2,filters=200,kernel_size=[6, 7],padding=\"same\",activation=tf.nn.relu)\n",
    "    print(conv3.shape)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[1,1], strides=2)\n",
    "    print(pool3.shape)\n",
    "    pool3_flat= tf.reshape(pool3,[-1,1*8*200])\n",
    "    logits = tf.layers.dense(inputs=pool3_flat, units=2, activation=tf.nn.softmax)\n",
    "    \n",
    "    predictions = {\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "      }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    eval_metric_ops = {\n",
    "          \"accuracy\": tf.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions[\"classes\"])\n",
    "      }\n",
    "    return tf.estimator.EstimatorSpec( mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/sentimentanalysis', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000018ACAD42898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "analyzer = tf.estimator.Estimator(model_fn=cnnmodel, model_dir=\"/tmp/sentimentanalysis\")\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "(10000, 5, 60, 50)\n",
      "(10000, 2, 30, 50)\n",
      "(10000, 2, 30, 100)\n",
      "(10000, 1, 15, 100)\n",
      "(10000, 1, 15, 200)\n",
      "(10000, 1, 8, 200)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/sentimentanalysis\\model.ckpt-12\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 12 into /tmp/sentimentanalysis\\model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[0.50080805 0.49919195]\n",
      " [0.50026514 0.49973486]\n",
      " [0.50069666 0.49930334]\n",
      " ...\n",
      " [0.50015769 0.49984231]\n",
      " [0.50042884 0.49957116]\n",
      " [0.5000636  0.4999364 ]]\n",
      "INFO:tensorflow:loss = 0.6932361721992493, step = 13\n",
      "INFO:tensorflow:Saving checkpoints for 16 into /tmp/sentimentanalysis\\model.ckpt.\n",
      "WARNING:tensorflow:From C:\\Users\\anoth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /tmp/sentimentanalysis\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 22 into /tmp/sentimentanalysis\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6932040452957153.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x18acad42438>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": traindata},\n",
    "    y=trainlabel,\n",
    "    batch_size=10000,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "analyzer.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=10,\n",
    "    hooks=[logging_hook]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "(?, 5, 60, 50)\n",
      "(?, 2, 30, 50)\n",
      "(?, 2, 30, 100)\n",
      "(?, 1, 15, 100)\n",
      "(?, 1, 15, 200)\n",
      "(?, 1, 8, 200)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-27T23:47:59Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/sentimentanalysis\\model.ckpt-22\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-27-23:48:39\n",
      "INFO:tensorflow:Saving dict for global step 22: accuracy = 0.4659, global_step = 22, loss = 0.6932046\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22: /tmp/sentimentanalysis\\model.ckpt-22\n",
      "{'accuracy': 0.4659, 'loss': 0.6932046, 'global_step': 22}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": testdata},\n",
    "    y=testlabel,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "eval_results = analyzer.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
